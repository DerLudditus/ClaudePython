<!DOCTYPE html>
<html lang="en"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0" /><title>How to remove duplicates from a list in Python</title>
<link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@700&amp;family=Literata:wght@400;700&amp;family=Fira+Code:wght@400;700&amp;display=swap" rel="stylesheet" type="text/css" />
  <link href="styles.css" rel="stylesheet" type="text/css" />
</head><body id="top">
  <script src="script.js"></script>
<h1><a href="index.html">How to remove duplicates from a list in Python</a></h1><div><div><div><p>Removing duplicate elements from Python lists is a common data cleaning task that developers encounter regularly. Python provides multiple built-in methods and techniques to efficiently handle duplicate values while maintaining list order and data integrity.</p><p>This guide covers proven techniques for duplicate removal, with practical examples and performance tips. All code examples were created with <a href="https://claude.ai/">Claude</a>, an AI assistant built by Anthropic.</p><h2>Using <code>set()</code> to remove duplicates</h2><pre><code>numbers = [1, 2, 3, 2, 1, 4, 5, 4]
unique_numbers = list(set(numbers))
print(unique_numbers)</code></pre><pre><code>[1, 2, 3, 4, 5]</code></pre><p>The <code>set()</code> function provides the fastest way to remove duplicates from a Python list. Sets store only unique values by design, automatically discarding duplicates during conversion. Converting the list to a set and back to a list creates a new sequence containing just the unique elements.</p><p>This approach offers key advantages for data cleaning:</p><ul><li>Maintains O(n) time complexity even with large lists</li><li>Handles any hashable data type including numbers and strings</li><li>Requires minimal code compared to manual filtering methods</li></ul><p>One important consideration: <code>set()</code> does not preserve the original order of elements. If maintaining sequence order matters for your use case, you'll need to explore alternative methods.</p><h2>Basic techniques for removing duplicates</h2><p>While <code>set()</code> excels at speed, Python offers several order-preserving methods to remove duplicates—from basic <code>for</code> loops to elegant <code>dict.fromkeys()</code> solutions.</p><h3>Using a <code>for</code> loop to preserve order</h3><pre><code>numbers = [1, 2, 3, 2, 1, 4, 5, 4]
unique_numbers = []
for num in numbers:
    if num not in unique_numbers:
        unique_numbers.append(num)
print(unique_numbers)</code></pre><pre><code>[1, 2, 3, 4, 5]</code></pre><p>This straightforward approach uses a <code>for</code> loop to iterate through the original list while building a new list of unique elements. The <code>not in</code> operator checks if each number already exists in <code>unique_numbers</code> before adding it.</p><ul><li>Preserves the original order of elements, unlike the <code>set()</code> method</li><li>Works with both hashable and unhashable data types</li><li>Simple to understand and modify for custom filtering logic</li></ul><p>While this method requires more code than using <code>set()</code>, it offers better control over the deduplication process. The trade-off comes in performance. The <code>not in</code> check becomes slower with larger lists because it must scan the entire <code>unique_numbers</code> list for each element.</p><h3>Using list comprehension with a tracking set</h3><pre><code>numbers = [1, 2, 3, 2, 1, 4, 5, 4]
seen = set()
unique_numbers = [x for x in numbers if not (x in seen or seen.add(x))]
print(unique_numbers)</code></pre><pre><code>[1, 2, 3, 4, 5]</code></pre><p>This elegant solution combines list comprehension with a tracking set to maintain element order while achieving better performance than a basic loop. The <code>seen</code> set efficiently tracks encountered elements, while the list comprehension creates the final unique list.</p><p>The clever part lies in the condition <code>not (x in seen or seen.add(x))</code>. It leverages Python's short-circuit evaluation and the fact that <code>add()</code> returns <code>None</code>. Here's how it works:</p><ul><li>When an element is first encountered, it's not in <code>seen</code>. The <code>add()</code> method adds it and returns <code>None</code></li><li>For duplicates, the <code>in</code> check returns <code>True</code> immediately, skipping the element</li><li>This approach preserves order while maintaining good performance for larger lists</li></ul><p>The result combines the speed benefits of sets with the order preservation of loops, making it an excellent choice for most deduplication tasks.</p><h3>Using the <code>dict.fromkeys()</code> method</h3><pre><code>numbers = [1, 2, 3, 2, 1, 4, 5, 4]
unique_numbers = list(dict.fromkeys(numbers))
print(unique_numbers)</code></pre><pre><code>[1, 2, 3, 4, 5]</code></pre><p>The <code>dict.fromkeys()</code> method creates a dictionary using list elements as keys. Since dictionary keys must be unique, this automatically removes duplicates. Converting back to a list with <code>list()</code> produces the final deduplicated sequence.</p><ul><li>Preserves the original order of elements in Python 3.7+ due to dictionary insertion order guarantees</li><li>Offers a clean one-line solution that's more readable than complex loops</li><li>Performs efficiently by leveraging dictionary's O(1) lookup time</li></ul><p>This approach strikes an excellent balance between code simplicity and performance. It works particularly well for basic data types like numbers and strings that can serve as dictionary keys.</p><h2>Advanced techniques for removing duplicates</h2><p>Beyond Python's built-in methods, specialized libraries like <code>collections</code>, <code>pandas</code>, and <code>NumPy</code> offer powerful tools for handling duplicate values in complex data structures.</p><h3>Using <code>OrderedDict</code> from collections</h3><pre><code>from collections import OrderedDict
numbers = [1, 2, 3, 2, 1, 4, 5, 4]
unique_numbers = list(OrderedDict.fromkeys(numbers))
print(unique_numbers)</code></pre><pre><code>[1, 2, 3, 4, 5]</code></pre><p>The <code>OrderedDict</code> approach combines the benefits of dictionaries with guaranteed order preservation. Similar to <code>dict.fromkeys()</code>, it creates a dictionary using list elements as keys while maintaining their original sequence.</p><ul><li>The <code>fromkeys()</code> method automatically discards duplicates since dictionary keys must be unique</li><li>Converting back to a list with <code>list()</code> produces the final deduplicated sequence</li><li>This method works reliably across all Python versions. Unlike regular dictionaries which only preserve order in Python 3.7+</li></ul><p>While <code>OrderedDict</code> requires importing from the <code>collections</code> module, it provides a dependable solution when maintaining element order is crucial. The slight performance overhead compared to regular dictionaries rarely impacts real-world applications.</p><h3>Using pandas <code>drop_duplicates()</code> method</h3><pre><code>import pandas as pd
data = [(1, &#x27;a&#x27;), (2, &#x27;b&#x27;), (1, &#x27;a&#x27;), (3, &#x27;c&#x27;)]
df = pd.DataFrame(data, columns=[&#x27;num&#x27;, &#x27;letter&#x27;])
unique_rows = df.drop_duplicates().values.tolist()
print(unique_rows)</code></pre><pre><code>[[1, &#x27;a&#x27;], [2, &#x27;b&#x27;], [3, &#x27;c&#x27;]]</code></pre><p>Pandas offers a powerful solution for removing duplicates from complex data structures. The <code>drop_duplicates()</code> method efficiently handles duplicate rows in a DataFrame, considering all columns by default when determining uniqueness.</p><ul><li>The example creates a DataFrame with paired number-letter data, where one pair (1, 'a') appears twice</li><li>Converting the data to a DataFrame enables pandas' optimized duplicate detection</li><li>The <code>values.tolist()</code> chain converts the deduplicated DataFrame back into a familiar Python list format</li></ul><p>This approach particularly shines when working with tabular data or when you need to remove duplicates based on multiple columns. Pandas handles the heavy lifting of comparing complex data structures while maintaining excellent performance.</p><h3>Using NumPy's <code>unique()</code> function with index tracking</h3><pre><code>import numpy as np
numbers = [4, 1, 3, 2, 1, 4, 5, 3]
unique_indices = np.unique(numbers, return_index=True)[1]
unique_in_order = [numbers[i] for i in sorted(unique_indices)]
print(unique_in_order)</code></pre><pre><code>[4, 1, 3, 2, 5]</code></pre><p>NumPy's <code>unique()</code> function with <code>return_index=True</code> returns both unique values and their first occurrence positions in the original array. The code leverages these indices to maintain the original order while removing duplicates.</p><ul><li>The <code>unique_indices</code> variable captures the positions where each unique number first appears in the list</li><li>Sorting these indices with <code>sorted()</code> ensures elements appear in their original sequence</li><li>The list comprehension <code>[numbers[i] for i in sorted(unique_indices)]</code> rebuilds the list using only the first occurrences</li></ul><p>This approach combines NumPy's efficient array operations with Python's built-in sorting capabilities. It works particularly well for numerical data where maintaining the original order matters.</p><h3>Finding unique words in a text</h3><p>Text processing often requires extracting unique words while preserving their original sequence, and Python's list comprehension with a tracking set delivers an elegant solution for this common task.</p><pre><code>text = &quot;The quick brown fox jumps over the lazy dog. The dog was not amused.&quot;
words = text.lower().replace(&#x27;.&#x27;, &#x27;&#x27;).split()
seen = set()
unique_words = [word for word in words if not (word in seen or seen.add(word))]
print(unique_words)</code></pre><p>This code efficiently extracts unique words from a text string while preserving their original order. The process starts by converting the text to lowercase with <code>lower()</code> and removing periods with <code>replace()</code>. The <code>split()</code> function then creates a list of individual words.</p><ul><li>The <code>seen</code> set tracks encountered words</li><li>The list comprehension creates a new list containing only first occurrences</li><li>The condition <code>not (word in seen or seen.add(word))</code> cleverly combines checking and tracking in one step</li></ul><p>This technique proves especially useful when processing natural language text where maintaining the original word sequence matters. The solution balances readability with efficient memory usage.</p><h3>Removing duplicate users while keeping most recent data</h3><p>When managing user data, a common challenge involves retaining only the most recent record for each unique user ID while discarding outdated entries—this example demonstrates an elegant dictionary-based solution for deduplicating and updating user profiles.</p><pre><code>user_records = [
    {&quot;id&quot;: 101, &quot;name&quot;: &quot;Alice&quot;, &quot;timestamp&quot;: &quot;2023-01-15&quot;},
    {&quot;id&quot;: 102, &quot;name&quot;: &quot;Bob&quot;, &quot;timestamp&quot;: &quot;2023-01-16&quot;},
    {&quot;id&quot;: 101, &quot;name&quot;: &quot;Alice Smith&quot;, &quot;timestamp&quot;: &quot;2023-02-20&quot;},
    {&quot;id&quot;: 102, &quot;name&quot;: &quot;Robert&quot;, &quot;timestamp&quot;: &quot;2023-02-25&quot;}
]

latest_records = {}
for record in user_records:
    user_id = record[&quot;id&quot;]
    if user_id not in latest_records or record[&quot;timestamp&quot;] &gt; latest_records[user_id][&quot;timestamp&quot;]:
        latest_records[user_id] = record

unique_users = list(latest_records.values())
print([f&quot;{user[&#x27;id&#x27;]}: {user[&#x27;name&#x27;]}&quot; for user in unique_users])</code></pre><p>This code efficiently handles user profile updates by maintaining only the most recent record for each unique user ID. The <code>latest_records</code> dictionary stores user records with their IDs as keys, automatically overwriting older entries when newer timestamps appear.</p><p>The core logic lies in the <code>if</code> condition. It checks two scenarios: either the user ID doesn't exist yet in <code>latest_records</code>, or the current record has a more recent timestamp than the stored one. When either condition is true, the code updates the dictionary with the current record.</p><ul><li>Uses dictionary's O(1) lookup time for efficient duplicate checking</li><li>Preserves the most recent data by comparing timestamp strings</li><li>Outputs a clean list of unique users with their latest information</li></ul><h2>Common errors and challenges</h2><p>Python developers often encounter three key challenges when removing duplicates: handling unhashable types, preserving element order, and managing case sensitivity.</p><h3>Dealing with unhashable types like <code>list</code> when removing duplicates</h3><p>Python's <code>set()</code> function cannot directly handle lists as elements because lists are mutable. When you try to convert nested lists into a set, Python raises a <code>TypeError</code>. The code below demonstrates this common pitfall.</p><pre><code>data = [[1, 2], [3, 4], [1, 2], [5, 6]]
unique_data = list(set(data))
print(unique_data)</code></pre><p>The code fails because Python can't hash lists as elements within a <code>set()</code>. Lists can change after creation, making them incompatible with Python's hash-based data structures. Let's examine a working solution in the code below.</p><pre><code>data = [[1, 2], [3, 4], [1, 2], [5, 6]]
unique_data = []
for item in data:
    if item not in unique_data:
        unique_data.append(item)
print(unique_data)</code></pre><p>The solution uses a simple <code>for</code> loop with <code>not in</code> checks to handle unhashable types like lists. This approach works because it compares list elements directly instead of trying to hash them. While slightly slower than <code>set()</code>, it reliably removes duplicates while preserving the original order.</p><ul><li>Watch for this issue when working with nested data structures like lists of lists or dictionaries</li><li>Consider converting unhashable elements to tuples if order preservation isn't critical</li><li>Remember that modifying list elements after deduplication could create unintended duplicates</li></ul><p>This pattern becomes especially important when processing complex data structures from APIs or file imports that contain nested arrays or objects.</p><h3>Maintaining order when using <code>set()</code> to remove duplicates</h3><p>The <code>set()</code> function efficiently removes duplicates but randomizes the sequence of elements in your list. This behavior can create unexpected results when order matters. The output below demonstrates how Python's set operations shuffle the original sequence.</p><pre><code>numbers = [10, 5, 3, 5, 10, 8]
unique_numbers = list(set(numbers))
print(unique_numbers)</code></pre><p>The <code>set()</code> operation discards the original sequence, outputting elements in an arbitrary order that may differ between Python runs. The code below demonstrates a reliable solution that maintains the initial ordering.</p><pre><code>from collections import OrderedDict
numbers = [10, 5, 3, 5, 10, 8]
unique_numbers = list(OrderedDict.fromkeys(numbers))
print(unique_numbers)</code></pre><p>The <code>OrderedDict</code> solution elegantly preserves element sequence while removing duplicates. Unlike <code>set()</code>, which randomizes order, <code>OrderedDict.fromkeys()</code> maintains the original position of each element in the list. This approach works consistently across all Python versions.</p><ul><li>Watch for order preservation when deduplicating sorted data or sequences where element position carries meaning</li><li>Consider using this method when processing user inputs, log files, or time-series data where sequence matters</li><li>The slight performance overhead rarely impacts real-world applications</li></ul><h3>Removing duplicates in a case-insensitive manner</h3><p>Python's <code>set()</code> function treats strings with different letter cases as distinct elements. When deduplicating text data, this default case-sensitive behavior often produces unexpected results by keeping both uppercase and lowercase versions of the same word.</p><pre><code>words = [&quot;apple&quot;, &quot;Apple&quot;, &quot;banana&quot;, &quot;orange&quot;]
unique_words = list(set(words))
print(unique_words)</code></pre><p>The <code>set()</code> function treats "apple" and "Apple" as completely different strings. This creates duplicate entries in the final output when we only want one version of each word. Let's examine a solution that handles case differences properly.</p><pre><code>words = [&quot;apple&quot;, &quot;Apple&quot;, &quot;banana&quot;, &quot;orange&quot;]
seen = set()
unique_words = []
for word in words:
    if word.lower() not in seen:
        seen.add(word.lower())
        unique_words.append(word)
print(unique_words)</code></pre><p>The solution uses a tracking set to store lowercase versions of words while maintaining the original case in the output list. The <code>seen</code> set checks for duplicates by converting each word to lowercase with <code>word.lower()</code>. When a new word appears, both the lowercase version enters the tracking set and the original word joins the output list.</p><ul><li>Watch for case sensitivity when processing user input or text data from multiple sources</li><li>Consider this approach for search functionality where "Apple" and "apple" should match</li><li>Remember that the first occurrence of a word preserves its original capitalization</li></ul><p>This pattern proves especially useful when cleaning data from user forms, processing search queries, or standardizing text datasets where case variations shouldn't create duplicates.</p></div></div></div><h2>FAQs</h2><div><div><h3>How do you remove duplicates while preserving the original order of elements?</h3><div><div><div><p>To remove duplicates while preserving order, you can use a <code>set</code> to track seen elements and build a new list with unique values. The <code>set</code> provides constant-time lookups to check if we've encountered an element before.</p><ul><li>Create an empty <code>set</code> to store seen values</li><li>Iterate through the original list in order</li><li>For each element, check if it exists in the <code>set</code></li><li>If not in the <code>set</code>, add it to both our result list and the <code>set</code></li></ul><p>This approach maintains the original sequence because we process elements in their initial order. It achieves O(n) time complexity since <code>set</code> operations are constant time.</p></div></div></div></div></div><div><div><h3>What&#x27;s the difference between using set() and dict.fromkeys() for duplicate removal?</h3><div><div><div><p>While <code>set()</code> and <code>dict.fromkeys()</code> both remove duplicates, they serve different purposes. <code>set()</code> creates an unordered collection of unique elements, perfect for simple deduplication. <code>dict.fromkeys()</code> generates a dictionary where the input elements become keys—all mapped to the same default value.</p><p>The key distinction lies in data preservation. <code>set()</code> discards duplicates entirely. <code>dict.fromkeys()</code> maintains the original items as dictionary keys while adding the capability to associate values with them.</p></div></div></div></div></div><div><div><h3>Can you remove duplicates from a list containing unhashable types like dictionaries?</h3><div><div><div><p>You can't directly remove duplicates from a list of dictionaries using <code>set()</code> since dictionaries aren't hashable. However, you can convert dictionaries to immutable tuples first. Transform each dictionary into a tuple of sorted items using <code>tuple(sorted(d.items()))</code>.</p><p>This approach works because tuples of hashable elements are themselves hashable. After converting to a set of tuples to remove duplicates, transform the unique tuples back into dictionaries.</p></div></div></div></div></div><div><div><h3>How do you remove duplicates based on a specific condition or key?</h3><div><div><div><p>Python's <code>dict.fromkeys()</code> efficiently removes duplicates while preserving order. For more control, combine a dictionary comprehension with a key function that specifies your deduplication criteria. This approach lets you define custom logic for determining what makes items unique.</p><ul><li>Create a key function that returns the values to compare</li><li>Use a dictionary comprehension with your key function to build a mapping</li><li>Extract the filtered values from the resulting dictionary</li></ul><p>This method works because dictionaries inherently maintain unique keys. The key function transforms your data into a form that captures the essence of what makes each item distinct.</p></div></div></div></div></div><div><div><h3>What happens to the original list when you use methods like set() to remove duplicates?</h3><div><div><div><p>The <code>set()</code> method creates an entirely new object without modifying your original list. When you convert a list to a set, Python generates a new data structure in memory that contains only unique elements. The original list remains unchanged unless you explicitly reassign it.</p><p>To permanently remove duplicates from your list, you'll need to convert the set back to a list and reassign it—a process that creates a third object in memory. This explains why <code>list(set())</code> is a common pattern for deduplication.</p></div></div></div></div></div><h2>🏠</h2></body></html>